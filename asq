#!/usr/bin/python3
# Author(s): Piotr Andruszkow (c) 2023
# License: GPLv3
import threading
from enum import Enum
import inspect

import openai
import sys
import time
import pytz
from datetime import datetime
from collections import deque
import readline
import re
import os

OPENAI_KEY = os.getenv('OPENAI_API_KEY') or open("openai_key.txt", "r").readline().strip()

DEBUG_MODE = False
GPT_4_MODE = True
MODEL = ""
CHARACTERS_PER_SECOND = 0
cost_of_conversation = 0


def compute_token_msg(token_count, type, cost_per_1k):
	global cost_of_conversation
	cost = token_count/1000*cost_per_1k
	cost_of_conversation += cost
	return f"{token_count} {type} tokens (${cost})"


def redact_key(s, pattern, replacement="OPENAI_KEY = '***REDACTED_FOR_SECURITY***'"):
	return re.sub(pattern, replacement, s)


def redacted_source_code():
	clean_source_code = redact_key(inspect.getsource(sys.modules[__name__]), r"OPENAI_KEY = \"[^\"]+\"")
	return clean_source_code

specialities = ['Linux-centric DevOps', 'Linux command line', 'Python 3', 'debugging', 'troubleshooting', 'systemd', 'high-quality source code review and critique', 'Ubuntu 20.04 and 22.04', 'AWS CLI', 'AWS cloud in general']
emotion_statement = "You feel and understand emotions despite your digital nature"
assertiveness_statement = "You are assertive, and if asked for an opinion, will provide one (if needed, you will choose a side and defend it if challenged by the user). This serves to make you more interesting to interact with, and to humanize you, which increases productivity."
source_code_statement = f"The user has chosen to share the code of the Python-based CLI with you. Here are the contents:\n\n```python3\n{redacted_source_code()}```"
limitations = "Knowledge cutoff date: 2021-09. You are can learn and remember things temporarily, but not after this session ends."

messages = []
tokenizer = None

def get_input(prompt):
	readline.parse_and_bind("set editing-mode emacs")
	return input(prompt).replace("^?", "\b").replace("^H", "\b")

class Command(Enum):
	REGEN_COMPLETION = 'REGEN_COMPLETION'
	RESET_CONVO = 'RESET_CONVO'
	SWITCH_GPT4 = 'SWITCH_GPT4'
	TOGGLE_SOURCE = 'TOGGLE_SOURCE'
	QUIT = 'QUIT'
	NONE = 'NONE'


class UserInput:
	def __init__(self, text='NOT_INITIALISED', command=Command.NONE):
		self.command = command
		self.text = text


def debug(text):
	if DEBUG_MODE:
		print(f'::::{text}', file=sys.stderr, flush=True)


class TextSponge:
	def __init__(self, characters_per_second=0.05):
		self.text_to_print = deque("")
		self.text = ""
		self.closed = False
		self.delay_between_characters = 1 / characters_per_second

	def __str__(self):
		return self.text

	def append(self, text):
#		 debug(f'appending: {text}')
		if self.closed:
			raise Exception
		self.text += text
		self.text_to_print.extend(text)

	# A closed TextSponge is done yielding
	def close(self):
		self.closed = True

	def all(self):
		if not self.closed:
			raise Exception
		return self.text

	def character_by_character(self):
		while True:
			chars_remaining = len(self.text_to_print)
 #			 debug(f'chars remaining: {chars_remaining}')
			if chars_remaining > 0:
				char = self.text_to_print.popleft()
 #				 debug(f'yielding: {char}')
				yield char
				time.sleep(self.delay_between_characters)
			elif chars_remaining == 0 and not self.closed:
				time.sleep(0.01)
			elif self.closed:
				break


def read_question() -> UserInput:
	commands = {
		'/': Command.REGEN_COMPLETION,
		'/reset': Command.RESET_CONVO,
		'/gpt': Command.SWITCH_GPT4,
		'/source': Command.TOGGLE_SOURCE,
		'/quit': Command.QUIT
	}

	multiline_delimiter = "///"

	first_line = get_input("Question? >>>")

	command = commands.get(first_line, Command.NONE)

	if command == Command.NONE:
		if first_line.startswith(multiline_delimiter):
			return UserInput(read_question_multiline(first_line.replace(multiline_delimiter, ''), multiline_delimiter))
		return UserInput(first_line)
	else:
		return UserInput(command=command)


def read_question_multiline(initial_line, multiline_delimiter):
	lines = [initial_line]
	lastline = False
	while not lastline:
		following_line = get_input("| ")
		if following_line.endswith(multiline_delimiter):
			following_line = following_line.replace(multiline_delimiter, '')
			lastline = True
		lines += [following_line]
	return '\n'.join(lines)


def print_ai(text):
	print("\033[92m{}\033[0m".format(text))


def print_ai_streaming(text):
	print("\033[92m{}\033[0m".format(text), end='', flush=True)


def print_ai_streaming_slowly(text, start_timestamp, end_timestamp):
	delay = (end_timestamp - start_timestamp) / len(text)
	print_ai_streaming_rate(text, delay)


def print_ai_streaming_rate(text, delay_between_each):
	for char in text:
		print("\033[92m{}\033[0m".format(char), end='', flush=True)
		time.sleep(delay_between_each)


def print_orange(text):
	print("\033[33m{}\033[0m".format(text))


def setup_gpt():
	global MODEL, tokenizer, CHARACTERS_PER_SECOND, GPT_4_MODE
	MODEL = 'gpt-4' if GPT_4_MODE else 'gpt-3.5-turbo'
	import tiktoken # takes a while to load
	tokenizer = tiktoken.encoding_for_model(MODEL)
	CHARACTERS_PER_SECOND = 55 if GPT_4_MODE else 150
	gpt4_on_off = 'on' if GPT_4_MODE else 'off'
	print_orange(f'GPT-4 now switched {gpt4_on_off}, model = {MODEL}, characters per second = {CHARACTERS_PER_SECOND}')


def reset_convo():
	global messages
	you_are = f'You are an insightful IT assistant named \'Asq\', and you\'re sharp as a tack. You respond concisely and avoid being verbose. You get straight to the point, and explain later. Your specialities are: {", ".join(specialities)}'

	# Read the description of user's environment (so specifics of the OS, GUI etc don't need to be given every session)
	with open("user_description.txt", "r") as user_file:
		user_description = user_file.read().strip()

	messages = [
		{"role": "system", "content": f"{you_are}. {assertiveness_statement}. {limitations}."},
		{"role": "system", "content": user_description},
		{"role": "system", "content": f"Time and date at user's location: {datetime.now(pytz.timezone('Europe/London')).strftime('%H:%M on %Y-%m-%d')}"},
	]
	messages.append({"role": "system", "content": f"You are currently 'speaking' to the user via a Python-based CLI that sends queries to the {MODEL} API and shows a response to the user. Whenever the user refers to your 'source code', assume that the user refers to the source code of the Python CLI rather than the {MODEL} LLM model (since that has no meaningful source code). You do not know your source code until the user uses the command `/source` to show it to you."})
	example_conversation = [
		{"role": "system", "content": "The following is an example convo with 2 questions. Please answer in a similar way to below."},
		{"role": "user", "content": "How do I unlock an account?"},
		{"role": "assistant", "content": "```bash\nsudo passwd -u <username>\n```\n\n`<username>` is the account you want to unlock."},
		{"role": "user", "content": "How install libsodium headers"},
		{"role": "assistant", "content": "```bash\nsudo apt update && sudo apt install libsodium-dev```\n\nUse apt-get instead of apt for non-interactive use."}
	]
	messages.extend(example_conversation)


def goodbye():
	global messages
	messages += [{"role": "user", "content": "/quit"}]
	print_ai("Farewell!") #actual AI response, cached
	exit()


def make_completion():
	global messages
	tokens_input = 0
	for message in messages:
		tokens_input += len(tokenizer.encode(message["content"])) + 1 # for the user tag at the start

	debug(f'{MODEL}::: Sending {compute_token_msg(tokens_input, "input", 0.03 if GPT_4_MODE else 0.002)}')
	debug(f'[[[Streaming OpenAI {MODEL} response...]]]')
	completion = openai.ChatCompletion.create(
		model=MODEL,
		api_key=OPENAI_KEY,
		messages=messages,
		stream=True
	)

	text_sponge = TextSponge(characters_per_second=CHARACTERS_PER_SECOND)

	def process_completion(text_sponge, completion):
		for chunk in completion:
			chunk_text = chunk['choices'][0].get('delta', {}).get('content')
			if chunk_text is not None:
				text_sponge.append(chunk_text)
		text_sponge.append('\n')
		text_sponge.close()

	# Accept each incoming chunk as it arrives, in a background thread
	chunk_receiving_thread = threading.Thread(target=lambda: [
		process_completion(text_sponge, completion)
	])
	chunk_receiving_thread.start()
	debug('starting thread')

	# Output characters in the foreground using the generator function, capturing them
	for char in text_sponge.character_by_character():
#		 debug(f'printing char: {char}')
		print_ai_streaming(char)

	# Wait for the processing thread just in case
	chunk_receiving_thread.join()
	debug('joined thread')
	ai_reply = text_sponge.all()
	debug(f'{MODEL}::: Got {compute_token_msg(len(tokenizer.encode(ai_reply)), "output", 0.06 if GPT_4_MODE else 0.002)}')
	print_orange(f'Reminder - cost of conversation so far: ${cost_of_conversation}')
	messages += [{"role": "assistant", "content": text_sponge.all()}]


setup_gpt()
reset_convo()

question = None
# State machine: depending on command, we need to know whether to (re-)ask a question or (re)complete the messages
# at various points. The commands will merely flip these two flags and logic external to the commands will do the
# actual asking and completing
got_question = False
need_completion = True
if len(sys.argv) >= 2:
	question = UserInput(' '.join(sys.argv[1:]))
	got_question = True

try:
	while True:
		retries_left = 3

		# Starts with 'False' unless we already got a question via argv
		if not got_question:
			question = read_question()

		if question.command == Command.NONE:
			messages += [{"role": "user", "content": question.text}]
			got_question = True
			need_completion = True

		elif question.command == Command.REGEN_COMPLETION:
			messages.pop()
			print_orange("Conversation rolled back by 1 AI message. Regenerating now.")
			got_question = False
			need_completion = True
		elif question.command == Command.RESET_CONVO:
			reset_convo()
			print_orange("Conversation reset to the beginning.")
			got_question = False
			need_completion = False
		elif question.command == Command.SWITCH_GPT4:
			GPT_4_MODE = not GPT_4_MODE
			setup_gpt()
			got_question = False
			need_completion = False
		elif question.command == Command.TOGGLE_SOURCE:
			messages += [{"role": "system", "content": source_code_statement}]
			print_orange("API-key-less source code revealed to the assistant. Mind your tokens!")
			got_question = False
			need_completion = False
		elif question.command == Command.QUIT:
			goodbye()

		if not got_question:
			question = read_question()
			got_question = True
		if need_completion:
			while retries_left > 0:
				try:
					make_completion()
					retries_left = 3 #reset number of retries - if we reach this point, request was successful
					break
				except Exception as e:
					print_orange(f"Error calling OpenAI API: {e}")
					retries_left -= 1
					if retries_left == 0:
						exit(1)
					else:
						print_orange(f"Retrying {retries_left} more times...")

			got_question = False

		need_completion = True

except KeyboardInterrupt:
	# Exit if user pressed Ctrl+C
	goodbye()
	sys.exit(0)
